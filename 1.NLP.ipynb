{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP : \n",
    "is the art and science which helps us extract information from text and use it in our computations and algorithms. \n",
    "\n",
    "Data Science -- usig data to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus or text corpus : \n",
    "is a large and structured set of texts, they are used to do statistical analysis and hypothesis testing,\n",
    "checking occurrences or validating linguistic rules within a specific language territory."
   ]
  },
  {
   "source": [
    "## ------------------------------------------------------\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Applications : "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Sentiment Analysis, 2.Topic Modeling, 3.Text Generation, 4.Chatbot, 5.Speech Recognition, 6.Machine Translator,\n",
    "7.Spell check, 8.Information extraction(Retrieval), 9.Keyword Search, 10.Advertisement matching, \n",
    "11.Document Clustering, 12.Text Summarisation, 13.Recommendation engine.     "
   ]
  },
  {
   "source": [
    "## ------------------------------------------------------\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Top NLP Library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. NLTK,  2.Spacy,  3.Flair,  4.Gensim,  5.Textblob,  6.Stanford Core NLP,  7.Pattern,  8.PyNLPl,  9.Scikit-learn,  10.Polyglot\n",
    "\n",
    "--> BERT - Transformers "
   ]
  },
  {
   "source": [
    "## ------------------------------------------------------\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic tasks of NLP : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Tokenization                                                                                                            \n",
    "2.Stopwords                                                                                                                  \n",
    "3.Stemming                                                                                                                   \n",
    "4.Lemmatisation                                                                                                             \n",
    "5.Part-of-Speech Tagging                                                                                                    \n",
    "6.Chunking , Chinking                                                                                                         \n",
    "7.Named Entity Recognition \n",
    "\n",
    "8.Word Embeddings : \n",
    "    types : 1.Count vectorizer, 2.TF-IDF, 3.Co-Occurrence matrix, 4.Word2Vec (1.CBOW, 2.Skip-Gram)  \n",
    "            5.Glove (Global Vectors for Word Representation) its pre-trained , 6.FastText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization :                                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.Break a complex sentence into words                                                                                \n",
    "b.Understand the importance of each  word w.r.t. the sentence                                                           \n",
    "c.Produce a structural description on an input sentence                                                                 \n",
    "d.It has Bigrams(Token of two consecutive written words), Trigrams(3 consecutive words), Ngrams(n no. of consecutive words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing -- is form of grouping things\n",
    "# corpora -- is body of text (ex: medical journals, presidential speeches)\n",
    "\n",
    "# lexicon -- words and their meanings like : dict but in different fields \n",
    "# (ex: investor speak 'bull' - means bullish ,someone who is positive about the market )\n",
    "# (ex: person speak 'bull' in english 'bull' , means animal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great and The Python is awesome.', 'The sky is pink-bluish']\n",
      "Hello\n",
      "Mr.\n",
      "Smith\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "doing\n",
      "today\n",
      "?\n",
      "The\n",
      "weather\n",
      "is\n",
      "great\n",
      "and\n",
      "The\n",
      "Python\n",
      "is\n",
      "awesome\n",
      ".\n",
      "The\n",
      "sky\n",
      "is\n",
      "pink-bluish\n"
     ]
    }
   ],
   "source": [
    "import nltk                                                                      # nltk.download() \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example_text = '''Hello Mr. Smith, how are you doing today? \n",
    "                The weather is great and The Python is awesome. The sky is pink-bluish'''\n",
    "\n",
    "print(sent_tokenize(example_text))               # simply print\n",
    "\n",
    "for i in word_tokenize(example_text):            # or use like for loop \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os                                       # Using corpora folder\n",
    "import nltk\n",
    "import nltk.corpus                         \n",
    "\n",
    "print(nltk.__file__)    # it shows file loc. for init file of corpora folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet', 'wordnet.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n"
     ]
    }
   ],
   "source": [
    "# shows all files of corpora folder, we can use files for analysis & text\n",
    "\n",
    "print(os.listdir(nltk.data.find(\"corpora\")))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gives all files of gutenberg folder from corpora\n",
    "\n",
    "nltk.corpus.gutenberg.fileids()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')   # givses elments of file\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
     ]
    }
   ],
   "source": [
    "for word in hamlet[:500]:                 # taking 1st 500 words\n",
    "    print(word, sep=' ', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "hamlet_tokens = word_tokenize(str(hamlet[:20]))\n",
    "# hamlet_tokens\n",
    "len(hamlet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist      # frquency distribution\n",
    "fdist = FreqDist()                      # finding wordcount of all words in the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({\"'\": 24, ',': 19, '[': 2, ']': 2, '.': 2, \"'the\": 1, \"'tragedie\": 1, \"'of\": 1, \"'hamlet\": 1, \"'by\": 1, ...})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in hamlet_tokens:\n",
    "    fdist[word.lower()]+=1          # wordcount of all words\n",
    "fdist                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 24),\n",
       " (',', 19),\n",
       " ('[', 2),\n",
       " (']', 2),\n",
       " ('.', 2),\n",
       " (\"'the\", 1),\n",
       " (\"'tragedie\", 1),\n",
       " (\"'of\", 1),\n",
       " (\"'hamlet\", 1),\n",
       " (\"'by\", 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10 = fdist.most_common(10)           # gives most common words used\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "\n",
    "hamlet_blank = blankline_tokenize(str(hamlet))     \n",
    "len(hamlet_blank)        # gives how many paragraph we have and what all paragraphs are separated by new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization :  ['The', 'best', 'and', 'most', 'beautiful', 'thing', 'in', 'the', 'world', 'can', 'not', 'be', 'seen', 'or', 'touched', ',', 'they', 'must', 'be', 'felt', 'with', 'the', 'heart', '.']\n",
      "--------------------------------------------------\n",
      "Bigrams :  [('The', 'best'), ('best', 'and'), ('and', 'most'), ('most', 'beautiful'), ('beautiful', 'thing'), ('thing', 'in'), ('in', 'the'), ('the', 'world'), ('world', 'can'), ('can', 'not'), ('not', 'be'), ('be', 'seen'), ('seen', 'or'), ('or', 'touched'), ('touched', ','), (',', 'they'), ('they', 'must'), ('must', 'be'), ('be', 'felt'), ('felt', 'with'), ('with', 'the'), ('the', 'heart'), ('heart', '.')]\n",
      "--------------------------------------------------\n",
      "trigrams :  [('The', 'best', 'and'), ('best', 'and', 'most'), ('and', 'most', 'beautiful'), ('most', 'beautiful', 'thing'), ('beautiful', 'thing', 'in'), ('thing', 'in', 'the'), ('in', 'the', 'world'), ('the', 'world', 'can'), ('world', 'can', 'not'), ('can', 'not', 'be'), ('not', 'be', 'seen'), ('be', 'seen', 'or'), ('seen', 'or', 'touched'), ('or', 'touched', ','), ('touched', ',', 'they'), (',', 'they', 'must'), ('they', 'must', 'be'), ('must', 'be', 'felt'), ('be', 'felt', 'with'), ('felt', 'with', 'the'), ('with', 'the', 'heart'), ('the', 'heart', '.')]\n",
      "--------------------------------------------------\n",
      "ngrams :  [('The', 'best', 'and', 'most'), ('best', 'and', 'most', 'beautiful'), ('and', 'most', 'beautiful', 'thing'), ('most', 'beautiful', 'thing', 'in'), ('beautiful', 'thing', 'in', 'the'), ('thing', 'in', 'the', 'world'), ('in', 'the', 'world', 'can'), ('the', 'world', 'can', 'not'), ('world', 'can', 'not', 'be'), ('can', 'not', 'be', 'seen'), ('not', 'be', 'seen', 'or'), ('be', 'seen', 'or', 'touched'), ('seen', 'or', 'touched', ','), ('or', 'touched', ',', 'they'), ('touched', ',', 'they', 'must'), (',', 'they', 'must', 'be'), ('they', 'must', 'be', 'felt'), ('must', 'be', 'felt', 'with'), ('be', 'felt', 'with', 'the'), ('felt', 'with', 'the', 'heart'), ('with', 'the', 'heart', '.')]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "\n",
    "string = \"The best and most beautiful thing in the world can not be seen or touched, they must be felt with the heart.\"\n",
    "quote_tokens = nltk.word_tokenize(string)          \n",
    "print('tokenization : ', quote_tokens)                                          # 1. we tokenize into words\n",
    "print(50*\"-\")\n",
    "\n",
    "quote_bigrams = list(nltk.bigrams(quote_tokens))          # 2. making bigrams of it , it is in list form so using list\n",
    "print('Bigrams : ', quote_bigrams)\n",
    "print(50*\"-\")\n",
    "\n",
    "quote_trigrams = list(nltk.trigrams(quote_tokens))      # 3. making trigrams of it  \n",
    "print('trigrams : ', quote_trigrams)\n",
    "print(50*\"-\")\n",
    "\n",
    "quote_ngrams = list(nltk.ngrams(quote_tokens, 4))      # 3. ngrams , we have to give no. for N like here we use 4 here(any no.) \n",
    "print('ngrams : ', quote_ngrams)\n",
    "print(50*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopwords :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk has its own list of stopwords, we can import from nltk.corpus they are helpful in processing sentences, but not helpful in processing of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is an example showing off stop word filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))   # english - is an lang. of which stopwordS are using word_tokenize\n",
    "# print(stop_words)    # just to check print what are stop_words = filtered_sent(example_sent)\n",
    "\n",
    "words = word_tokenize(example_sent)\n",
    "\n",
    "# filtered_sent = []                  \n",
    "# for w in words:\n",
    "#     if w not in stop_words:\n",
    "#         filtered_sent.append(w)\n",
    "\n",
    "# print(filtered_sent)    \n",
    "\n",
    "\n",
    "# or use list comprehension instaead of for loop\n",
    "filtered_sent = [w for w in words if w not in stop_words]\n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords           # gives list of stopwords\n",
    "\n",
    "len(stopwords.words('english'))         # remove - len() and use print to see the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most of the words are stopwords and punctuation hence can be removed , Now use 'compile()' from 're' module to create a string that matches any digit or special character and then see how we can remove these stopewords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "punctuation = re.compile(r'-.?|:;()|0-9]')\n",
    "\n",
    "post_puctuation = []\n",
    "for words in hamlet_tokens:\n",
    "    word = punctuation.sub(\"\",words)\n",
    "    if len(word)>0:\n",
    "        post_puctuation.append(word)\n",
    "        \n",
    "# post_puctuation                        # see there are no stopwords here in the particular given output. see list remove len \n",
    "len(post_puctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemming : \n",
    "Normalize words into its base (or root) form. \n",
    "It fixes pre-fix and suffix both, result sometimes may not be likely.\n",
    "Its a rule based process of stripping(removing) the suffixes. (like: ing, ly, es, s)\n",
    "        \n",
    "Stemming : used where meaning of words not important (ex: spam detection)\n",
    "\n",
    "3 types : PorterStemmer , LancasterStemmer , SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "when\n",
      "you\n",
      "are\n",
      "python\n",
      "and\n",
      "do\n",
      "grey\n",
      "python\n",
      "exampl\n",
      ".\n",
      "the\n",
      "python\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "# for w in example_words:          # stemming of 'example_words' , here we have words in it.\n",
    "#     print(ps.stem(w))\n",
    "\n",
    "\n",
    "new_text = \"It is very important to be pythonly when you are pythoner and doing grey pythoned examples. The pythoning oncely.\"\n",
    "words = word_tokenize(new_text)     # Here we have sentence, so 1st tokenize sentence into words , 2nd stemming of words.\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "given:given\n",
      "giving:give\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words_to_stem = ['give', 'given', 'giving', 'gave']\n",
    "for words in words_to_stem:\n",
    "    print(words + ':' + ps.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "given:giv\n",
      "giving:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer              # Lancaster is more aggressive than Porter Stemmer.\n",
    "ls = LancasterStemmer()\n",
    "for words in words_to_stem:\n",
    "    print(words + ':' + ls.stem(words))         # Lancaster : use it as how many times (giv) in words used , for other : Porter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SnowballStemmer : in this we have to specify  language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "given:giv\n",
      "giving:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(language='english')      # but in this we have to specify - language\n",
    "\n",
    "for words in words_to_stem:\n",
    "    print(words + ':' + ls.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words               Porter Stemmer      Lancaster Stemmer   Snowball Stemmer\n",
      "   \n",
      "enemy               enemi               enemy               enemi               \n",
      "fellowship          fellowship          fellow              fellowship          \n",
      "friends             friend              friend              friend              \n",
      "stabilise           stabilis            stabl               stabilis            \n",
      "misunderstanding    misunderstand       misunderstand       misunderstand       \n",
      "sunlight            sunlight            sunlight            sunlight            \n",
      "football            footbal             footbal             footbal             \n",
      "pedler              pedler              pedl                pedler              \n",
      "roadster            roadster            roadst              roadster            \n",
      "awkward             awkward             awkward             awkward             \n"
     ]
    }
   ],
   "source": [
    "word_list = ['enemy','fellowship','friends','stabilise','misunderstanding','sunlight','football','pedler','roadster','awkward']\n",
    "print(\"{0:20}{1:20}{2:20}{3:20}\".format('Words', 'Porter Stemmer', 'Lancaster Stemmer', 'Snowball Stemmer''\\n'))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}{3:20}\".format(word, ps.stem(word), ls.stem(word), ss.stem(word)))    # All 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatization : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consider the morphological anlysis of word, using a detailed dictionary which the algorithm can look into.\n",
    "a.Groups together different inflected forms of a word , called Lemma.  \n",
    "b.somehow similar to stemming , as it maps several words into one common root.  \n",
    "c.Output of lemmatization is a proper word. \n",
    "d.EX : going, went, gone --> go \n",
    "\n",
    "Lemmatization : used when meaning of word is important (ex: Question-Answering app.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "given : given\n",
      "giving : giving\n",
      "gave : gave\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "words_to_stem = ['give', 'given', 'giving', 'gave']\n",
    "for words in words_to_stem:\n",
    "    print(words + ' : ' + lem.lemmatize(words))       # bocz, we not assign any pos-tags here and assumes all words- Noun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize('corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiply'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\"\n",
    "\n",
    "lem.lemmatize(word, \"v\")      # v- verb attribute\n",
    "# ps.stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. POS (Part of Speech) tagging : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag : \n",
    "gives Part of speech ( This means labeling words in a sentence as nouns, adjectives, verbs...etc.)\n",
    "# link : https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/\n",
    "# (PRP: personal pronoun, NN: Noun singular, IN: preposition, etc....)\n",
    "          \n",
    "( so many POS here google it - noun , adverb etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Timothy', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('natural', 'JJ')]\n",
      "[('when', 'WRB')]\n",
      "[('it', 'PRP')]\n",
      "[('comes', 'VBZ')]\n",
      "[('to', 'TO')]\n",
      "[('drawing', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "sent = \"Timothy is a natural when it comes to drawing\"       # 1st tokenize it , then took pos-tags\n",
    "sent_tokens = word_tokenize(sent)\n",
    "\n",
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))         # use [token] create a list for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('learning', 'VBG'),\n",
       " ('Natural', 'NNP'),\n",
       " ('Language', 'NNP'),\n",
       " ('Processing', 'VBG'),\n",
       " ('from', 'IN'),\n",
       " ('sentdex', 'NN')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "text = \"I am learning Natural Language Processing from sentdex\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "pos_tag(tokens)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer       # PunktSentenceTokenizer : is Unsupervise ML tokenizer  \n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_token = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_token.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:                      # use tokenized[:10] , bcoz of big file\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "# process_content()        # don't run it bcoz its big, run accordingly  , use ctrl+c to break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chunking : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking : \n",
    "is a process by which individual pieces of information are bound together into a meaningful whole.\n",
    "picks individual piece(or tokens) of information and group them into bigger pieces called chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "  'S/POS\n",
      "  (Chunk ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  (Chunk THE/NNP PRESIDENT/NNP)\n",
      "  :/:\n",
      "  (Chunk Thank/NNP)\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "(S\n",
      "  (Chunk Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  (Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  (Chunk Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Supreme/NNP Court/NNP)\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/VB\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  (Chunk called/VBD America/NNP)\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  founding/NN\n",
      "  ideals/NNS\n",
      "  and/CC\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n",
      "(S\n",
      "  Tonight/NN\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  comforted/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  hope/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  glad/JJ\n",
      "  reunion/NN\n",
      "  with/IN\n",
      "  the/DT\n",
      "  husband/NN\n",
      "  who/WP\n",
      "  was/VBD\n",
      "  taken/VBN\n",
      "  so/RB\n",
      "  long/RB\n",
      "  ago/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  grateful/JJ\n",
      "  for/IN\n",
      "  the/DT\n",
      "  good/JJ\n",
      "  life/NN\n",
      "  of/IN\n",
      "  (Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "  ./.)\n",
      "(S (/( (Chunk Applause/NNP) ./. )/))\n",
      "(S\n",
      "  (Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "  reacts/VBZ\n",
      "  to/TO\n",
      "  applause/VB\n",
      "  during/IN\n",
      "  his/PRP$\n",
      "  (Chunk State/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Union/NNP Address/NNP)\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (Chunk Capitol/NNP)\n",
      "  ,/,\n",
      "  (Chunk Tuesday/NNP)\n",
      "  ,/,\n",
      "  (Chunk Jan/NNP)\n",
      "  ./.)\n",
      "(S 31/CD ,/, 2006/CD ./.)\n",
      "(S\n",
      "  (Chunk White/NNP House/NNP photo/NN)\n",
      "  by/IN\n",
      "  (Chunk Eric/NNP DraperEvery/NNP time/NN)\n",
      "  I/PRP\n",
      "  'm/VBP\n",
      "  invited/JJ\n",
      "  to/TO\n",
      "  this/DT\n",
      "  rostrum/NN\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  'm/VBP\n",
      "  humbled/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  privilege/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  mindful/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  history/NN\n",
      "  we/PRP\n",
      "  've/VBP\n",
      "  seen/VBN\n",
      "  together/RB\n",
      "  ./.)\n",
      "(S\n",
      "  We/PRP\n",
      "  have/VBP\n",
      "  gathered/VBN\n",
      "  under/IN\n",
      "  this/DT\n",
      "  (Chunk Capitol/NNP dome/NN)\n",
      "  in/IN\n",
      "  moments/NNS\n",
      "  of/IN\n",
      "  national/JJ\n",
      "  mourning/NN\n",
      "  and/CC\n",
      "  national/JJ\n",
      "  achievement/NN\n",
      "  ./.)\n",
      "(S\n",
      "  We/PRP\n",
      "  (Chunk have/VBP served/VBN America/NNP)\n",
      "  through/IN\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  most/RBS\n",
      "  consequential/JJ\n",
      "  periods/NNS\n",
      "  of/IN\n",
      "  our/PRP$\n",
      "  history/NN\n",
      "  --/:\n",
      "  and/CC\n",
      "  it/PRP\n",
      "  has/VBZ\n",
      "  been/VBN\n",
      "  my/PRP$\n",
      "  honor/NN\n",
      "  to/TO\n",
      "  serve/VB\n",
      "  with/IN\n",
      "  you/PRP\n",
      "  ./.)\n",
      "(S\n",
      "  In/IN\n",
      "  a/DT\n",
      "  system/NN\n",
      "  of/IN\n",
      "  two/CD\n",
      "  parties/NNS\n",
      "  ,/,\n",
      "  two/CD\n",
      "  chambers/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  two/CD\n",
      "  elected/JJ\n",
      "  branches/NNS\n",
      "  ,/,\n",
      "  there/EX\n",
      "  will/MD\n",
      "  always/RB\n",
      "  be/VB\n",
      "  differences/NNS\n",
      "  and/CC\n",
      "  debate/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union                 # from corpus folder taking-> state_union.\n",
    "from nltk.tokenize import PunktSentenceTokenizer   # PunktSentenceTokenizer : is Unsupervise ML tokenizer  \n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_token = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_token.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:10]:                # used only 10 senetnces, Its big file run accordingly.\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk : {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "#             chunked.draw()\n",
    "            \n",
    "            print(chunked)\n",
    "            \n",
    "#             print(tagged)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('cat', 'NN')]), ('ate', 'VBD'), Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('mouse', 'NN')]), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), Tree('NP', [('fresh', 'JJ'), ('cheese', 'NN')])])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st install ghostscript  ( pip install ghostscript)\n",
    "\n",
    "# Now we create a grammar from Noun phrase and mention tags we want in our chunks phrase within the curly braces or {dict}\n",
    "\n",
    "new = \"The big cat ate the little mouse who was after fresh cheese\"\n",
    "new_token = nltk.pos_tag(word_tokenize(new))     # 1st word tokenize , then pos_tag\n",
    "new_token\n",
    "\n",
    "grammar_np = r\"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "chunk_parser = nltk.RegexpParser(grammar_np)   # Here created Regular expression matching string\n",
    "\n",
    "chunk_result = chunk_parser.parse(new_token)   # now parse our Noun phrase string to it.  \n",
    "chunk_result                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinking : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### chinking -- removal of something from group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chink PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP 'S/POS ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chink A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chink THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chink\n",
      "    THE/NNP\n",
      "    UNION/NNP\n",
      "    January/NNP\n",
      "    31/CD\n",
      "    ,/,\n",
      "    2006/CD\n",
      "    THE/NNP\n",
      "    PRESIDENT/NNP\n",
      "    :/:\n",
      "    Thank/NNP\n",
      "    you/PRP)\n",
      "  all/DT\n",
      "  (Chink ./.))\n",
      "(S\n",
      "  (Chink\n",
      "    Mr./NNP\n",
      "    Speaker/NNP\n",
      "    ,/,\n",
      "    Vice/NNP\n",
      "    President/NNP\n",
      "    Cheney/NNP\n",
      "    ,/,\n",
      "    members/NNS)\n",
      "  of/IN\n",
      "  (Chink Congress/NNP ,/, members/NNS)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chink\n",
      "    Supreme/NNP\n",
      "    Court/NNP\n",
      "    and/CC\n",
      "    diplomatic/JJ\n",
      "    corps/NN\n",
      "    ,/,\n",
      "    distinguished/JJ\n",
      "    guests/NNS\n",
      "    ,/,\n",
      "    and/CC\n",
      "    fellow/JJ\n",
      "    citizens/NNS\n",
      "    :/:)\n",
      "  Today/VB\n",
      "  (Chink our/PRP$ nation/NN)\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  (Chink ,/, graceful/JJ ,/, courageous/JJ woman/NN who/WP)\n",
      "  called/VBD\n",
      "  (Chink America/NNP)\n",
      "  to/TO\n",
      "  (Chink its/PRP$ founding/NN ideals/NNS and/CC)\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  (Chink noble/JJ dream/NN ./.))\n",
      "(S\n",
      "  (Chink Tonight/NN we/PRP)\n",
      "  are/VBP\n",
      "  comforted/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  (Chink hope/NN)\n",
      "  of/IN\n",
      "  a/DT\n",
      "  (Chink glad/JJ reunion/NN)\n",
      "  with/IN\n",
      "  the/DT\n",
      "  (Chink husband/NN who/WP)\n",
      "  was/VBD\n",
      "  taken/VBN\n",
      "  (Chink so/RB long/RB ago/RB ,/, and/CC we/PRP)\n",
      "  are/VBP\n",
      "  (Chink grateful/JJ)\n",
      "  for/IN\n",
      "  the/DT\n",
      "  (Chink good/JJ life/NN)\n",
      "  of/IN\n",
      "  (Chink Coretta/NNP Scott/NNP King/NNP ./.))\n",
      "(S (Chink (/( Applause/NNP ./. )/)))\n",
      "(S\n",
      "  (Chink President/NNP George/NNP W./NNP Bush/NNP)\n",
      "  reacts/VBZ\n",
      "  to/TO\n",
      "  applause/VB\n",
      "  during/IN\n",
      "  (Chink his/PRP$ State/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chink Union/NNP Address/NNP)\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (Chink Capitol/NNP ,/, Tuesday/NNP ,/, Jan/NNP ./.))\n",
      "(S (Chink 31/CD ,/, 2006/CD ./.))\n",
      "(S\n",
      "  (Chink White/NNP House/NNP photo/NN)\n",
      "  by/IN\n",
      "  (Chink Eric/NNP DraperEvery/NNP time/NN I/PRP)\n",
      "  'm/VBP\n",
      "  (Chink invited/JJ)\n",
      "  to/TO\n",
      "  this/DT\n",
      "  (Chink rostrum/NN ,/, I/PRP)\n",
      "  'm/VBP\n",
      "  humbled/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  (Chink privilege/NN ,/, and/CC mindful/NN)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chink history/NN we/PRP)\n",
      "  've/VBP\n",
      "  seen/VBN\n",
      "  (Chink together/RB ./.))\n",
      "(S\n",
      "  (Chink We/PRP)\n",
      "  have/VBP\n",
      "  gathered/VBN\n",
      "  under/IN\n",
      "  this/DT\n",
      "  (Chink Capitol/NNP dome/NN)\n",
      "  in/IN\n",
      "  (Chink moments/NNS)\n",
      "  of/IN\n",
      "  (Chink\n",
      "    national/JJ\n",
      "    mourning/NN\n",
      "    and/CC\n",
      "    national/JJ\n",
      "    achievement/NN\n",
      "    ./.))\n",
      "(S\n",
      "  (Chink We/PRP)\n",
      "  have/VBP\n",
      "  served/VBN\n",
      "  (Chink America/NNP)\n",
      "  through/IN\n",
      "  (Chink one/CD)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chink most/RBS consequential/JJ periods/NNS)\n",
      "  of/IN\n",
      "  (Chink our/PRP$ history/NN --/: and/CC it/PRP)\n",
      "  has/VBZ\n",
      "  been/VBN\n",
      "  (Chink my/PRP$ honor/NN)\n",
      "  to/TO\n",
      "  serve/VB\n",
      "  with/IN\n",
      "  (Chink you/PRP ./.))\n",
      "(S\n",
      "  In/IN\n",
      "  a/DT\n",
      "  (Chink system/NN)\n",
      "  of/IN\n",
      "  (Chink\n",
      "    two/CD\n",
      "    parties/NNS\n",
      "    ,/,\n",
      "    two/CD\n",
      "    chambers/NNS\n",
      "    ,/,\n",
      "    and/CC\n",
      "    two/CD\n",
      "    elected/JJ\n",
      "    branches/NNS\n",
      "    ,/,\n",
      "    there/EX\n",
      "    will/MD\n",
      "    always/RB)\n",
      "  be/VB\n",
      "  (Chink differences/NNS and/CC debate/NN ./.))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer       # PunktSentenceTokenizer : is Unsupervise ML tokenizer  \n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_token = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_token.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:10]:                # used only 10 senetnces, Its big file run accordingly.\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chink : {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "#             chunked.draw()\n",
    "            \n",
    "            print(chunked)\n",
    "            \n",
    "#             print(tagged)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Named Entity Recognition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER is a subtask to extract information that seeks to locate and classify named entity \n",
    "mentions in unstructured text into pre-defined categories such as the names, organizations, locations, values etc.                                       \n",
    "      \n",
    "NER:                                                                                                                       \n",
    "person name, company name, organisation name, location name, entity name, monetary value name called NER.\n",
    "\n",
    "a.Non-phase identification : \n",
    "deals with the extracting all noun phrases from text using dependency passing and POS tagging.\n",
    "\n",
    "b.Phrase calssification : \n",
    "all extarcted noun phrases are classified into respective categories (location, name, org. etc.)   \n",
    "\n",
    "c.Entity disambiguation : \n",
    "sometimes its possible the entities are mis-classified, hence creating a validation layer \n",
    "on top of result is useful and use of knowledge graphs can be exploited for this purpose         \n",
    "(Popular knowledge graphs are : Google Knowledge graph, IBM watson , wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer       # PunktSentenceTokenizer : is Unsupervise ML tokenizer  \n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_token = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_token.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:                        # Use tokenized[:50] , it big file  \n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            namedEnt = nltk.ne_chunk(tagged, binary = True)\n",
    "            print(namedEnt)\n",
    "            # namedEnt.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "# process_content()                   # Its big file, run accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION US/NNP)\n",
      "  President/NNP\n",
      "  stays/VBZ\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (FACILITY White/NNP House/NNP))\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "ne_sen = \"The US President stays in the White House\"\n",
    "ne_tokens = word_tokenize(ne_sen)\n",
    "ne_tags = nltk.pos_tag(ne_tokens)         # without pos_tags , its hard to detect NER\n",
    "ne_ner = ne_chunk(ne_tags)\n",
    "\n",
    "print(ne_ner)       # US: recognised as Organisation, White House: is club together as single entity and recognised as Facility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Word Embeddings :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text.\n",
    "#### word embedding : vector representation of a particular word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types : 1.Count Vectorizer ,  2.TF-IDF ,  3.Co-Occurrence matrix, 4.Word2Vec, 5.FastText, 6.Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Count-Vectorizer (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag of word: \n",
    "is a important step in text visualization or text preprocessing. \n",
    "we convert those texts into integer format before passing to a model.\n",
    "If we pass in int or float format model will be able to do calculations w.r.t. those texts and\n",
    "it can use them in a algorithm. Bag of words also called Document matrix.             \n",
    "It also important for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Steps: \n",
    "Cleaning the texts :                                                                                                        \n",
    "1.lower all texts(words or sentences) (ex: today & Today -model considers in separate way like 2 words)                        \n",
    "2.Tokenization                                                                                                              \n",
    " \n",
    "Creating Bag of Words (Document matrix) :                                                                                     \n",
    "3.Histogram (most important step)- means count the frequency(no.) of words present in the sentences.                 \n",
    "4.sort the Histogram- in descending order(no. of words(counts) greater is up and then lower - lower)                       \n",
    "5.filter the words- most frequent words(which means we take otherwise remove).                                                 \n",
    "6.Creating the matrix-> Bag of Words(Document matrix) \n",
    "In matrix, most frequent words(sorted words) created as columns and index are sentences. \n",
    "This matrix we create and this will be our input data provide to model. \n",
    "In sentiment analysis -> 1 more output column is created and other columns are (as input data).\n",
    "and model will predict for the future data whether data is pos or neg.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag of words - Problem :\n",
    "1.All words have same importance. \n",
    "(all 1 or 0 have same importance, but it should be like more positive more importance \n",
    " than less positive word , similar for negative.)\n",
    "\n",
    "2.No semantic information preserved.\n",
    "(if much importance is not given to any words then semantic information is lost \n",
    " for that partiular word in that sentence.)\n",
    "so we use -- > TF-IDF  for Document matrix , its better than - Bag of Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "paragraph = \"\"\"Being unique will require excellence, let us understand what is excellence in more detail. Excellence is a\n",
    "            self-imposed self-directed life-long process. Excellence is not by accident. It is a process, where an individual,\n",
    "            organization or nation, continuously strives to better oneself. The performance standards are set by themselves, \n",
    "            they work on their dreams with focus and are prepared to take calculated risks and do not get deterred by failures \n",
    "            as they move towards their dreams. Then they step up their dreams as they tend to reach the original targets. \n",
    "            They strive to work to their potential, in the process, they increase their performance thereby multiplying further\n",
    "            their potential and this is an unending life cycle phenomenon. They are not in competition with anyone else,\n",
    "            but themselves. That is the culture of excellence. Let me share an important experience from the life of the father \n",
    "            of the nation. Students ought to think and think well. They should do no wrong? Students should do everything to\n",
    "            build-up a new state of India which would be everybody's pride.\"\"\"\n",
    "\n",
    "# 1.Clean the texts(lowering texts , removing stopwords, (,) (.) lemmatizing it or stemming )\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "sent_token = nltk.sent_tokenize(paragraph)          # 1st tokenize paragraph into sentences\n",
    "\n",
    "corpus=[]      # after cleaning texts store in it , then we see differnce b/w sent_token and corpus.\n",
    "\n",
    "for i in range(len(sent_token)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sent_token[i])   # this removes all (,) (.) (?) , punctuations etc. which not reqd.\n",
    "    review = review.lower()   \n",
    "    review = review.split()   # getting a list of words\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]  # use -> lem.lemmatize(word)\n",
    "    review = ' '.join(review)  # after stemming joining all words into review\n",
    "    corpus.append(review)      # then append to corpus\n",
    "    \n",
    "# print(sent_token)     # to check differnece print both -> sent_token  and corpus \n",
    "# print(50*\"-\")        # just to separate both , also we can use -> lemmatization (lem.lemmatize(word)) \n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 69)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Now create  Bag of Words (Document matrix)- model   (it follows steps 3,4,5,6)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer   # count vectorizer - create -> Bag of words (matrix)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(corpus).toarray()   # fit-transform convert corpus into matrix and toarray() used to see it properly\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF : Term Frequency,  TF= No. of occurence of a word in doc. / Total no. of words in doc.                                      \n",
    "TF-IDF : TF*IDF\n",
    "\n",
    "some semantic information is preserved as uncommon words are given more importance than common words.                          \n",
    "    Ex: 'She is beautiful'  , beautiful - more importance than 'She' or 'is'. \n",
    "\n",
    "<img src='TF.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF : Inverse Document Frequency.  IDF = log(No. of documents / No.of documents containing that word)                          \n",
    "      here documents=sentence \n",
    "        \n",
    "<img src='IDF.PNG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Steps : \n",
    "1.Cleaning the texts. \n",
    "2.Creating the TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer       \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "paragraph = \"\"\"Being unique will require excellence, let us understand what is excellence in more detail. Excellence is a\n",
    "            self-imposed self-directed life-long process. Excellence is not by accident. It is a process, where an individual,\n",
    "            organization or nation, continuously strives to better oneself. The performance standards are set by themselves, \n",
    "            they work on their dreams with focus and are prepared to take calculated risks and do not get deterred by failures \n",
    "            as they move towards their dreams. Then they step up their dreams as they tend to reach the original targets. \n",
    "            They strive to work to their potential, in the process, they increase their performance thereby multiplying further\n",
    "            their potential and this is an unending life cycle phenomenon. They are not in competition with anyone else,\n",
    "            but themselves. That is the culture of excellence. Let me share an important experience from the life of the father \n",
    "            of the nation. Students ought to think and think well. They should do no wrong? Students should do everything to\n",
    "            build-up a new state of India which would be everybody's pride.\"\"\"\n",
    "\n",
    "\n",
    "# 1.Cleaning the texts\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lem.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 69)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.Creating the TF-IDF model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer()\n",
    "x = tv.fit_transform(corpus).toarray()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tv.fit(corpus)\n",
    "\n",
    "# x.vocabulary_      # gives vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Co-Occurrence matrix with a fixed context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words co-occurrence matrix : \n",
    "describes how words occur together that in turn captures the relationships between words. \n",
    "Words co-occurrence matrix is computed simply by counting how two or more words occur together in a given corpus.\n",
    "\n",
    "Ex:\n",
    "1.penny wise and pound foolish.  \n",
    "2.a penny saved is a penny earned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='co-occurrence-matrix1.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The table shows that \"a\" is followed twice by \"penny\" while words \"earned\", \"saved\", and \"wise\"  \n",
    "follows \"penny\" once in our corpus. Thus, \"earned\" is one out of three times probable to appear after penny. \n",
    "The count shown is called bigram frequency.\n",
    "It looks into only the next word from a current word.\n",
    "Remember this co-occurrence matrix is not the word vector representation that is generally used. \n",
    "\n",
    "Instead, this Co-occurrence matrix is decomposed using techniques like PCA, SVD etc. into factors and \n",
    "combination of these factors forms the word vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*Advantages of Co-occurrence Matrix : \n",
    "It preserves the semantic relationship between words. i.e man and woman tend to be closer than man and apple.  \n",
    "It uses SVD at its core, which produces more accurate word vector representations than existing methods. \n",
    "It uses factorization which is a well-defined problem and can be efficiently solved. \n",
    "It has to be computed once and can be used anytime once computed. In this sense, it is faster in comparison to others.  \n",
    "    \n",
    "*Disadvantages of Co-Occurrence Matrix :        \n",
    "It requires huge memory to store the co-occurrence matrix.  \n",
    "But, this problem can be circumvented by factorizing the matrix out of the system, \n",
    "for example in Hadoop clusters etc. and can be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Word2Vec( 1.CBOW , 2.Skip-Gram )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec can be used to find out the relations between words in a dataset, \n",
    "compute the similarity between them, or use the vector representation of those words \n",
    "as input for other applications such as text classification or clustering.\n",
    "\n",
    "For example : \n",
    "\"dog\", \"puppy\" and \"pup\" are often used in similar situations, \n",
    "with similar surrounding words like \"good\", \"fluffy\" or \"cute\", and \n",
    "according to Word2Vec they will therefore share a similar vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='word2vec.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-Word Embedding is used to compute similar words, Create a group of related words, \n",
    "Feature for text classification, NLP, Document clustering.  \n",
    "\n",
    "-Word2vec is a shallow two-layered neural network model to produce word embedding for better word representation.\n",
    "The shallow neural network consists of the only a hidden layer between input and output \n",
    "whereas deep neural network contains multiple hidden layers between input and output. \n",
    "\n",
    "-Word2vec represents words in vector space representation. \n",
    "Words are represented in the form of vectors and placement is done in such a way that \n",
    "similar meaning words appear together and dissimilar words are located far away. \n",
    "\n",
    "-Word2vec used 2 architectures Continuous Bag of words (CBOW) and skip gram.\n",
    "\n",
    "-'CBOW is several times faster than skip gram' \n",
    "and provides a better frequency for frequent words whereas                         \n",
    "skip gram needs a small amount of training data and represents even rare words or phrases.  \n",
    " \n",
    "-In CBOW, the current word is predicted using the window of surrounding context windows.\n",
    "-Skip-Gram approach is used to predict a sentence given an input word.  \n",
    "-Skip-gram and CBOW convert unsupervised representation to supervised form for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can also use googles pre-trained model. It contains word vectors for a vocabulary \n",
    "of 3 million words trained on around 100 billion words from the google news dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downlad from here : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "written the code only not run that bcoz its an big file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Here is how you can train your own word vectors using gensim\n",
    "\n",
    "# sentence_df=[['first','sentence'],['second','sentence']]\n",
    "# model2 = Word2Vec(sentences=sentence_df, size=100, window=5, min_count=5, workers=4, sg=0)  # sg=1 : skip-Gram , sg=0 : CBOW"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# getting word vectors of a word\n",
    "dog = model['dog']\n",
    "\n",
    "#performing king queen magic\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "\n",
    "#picking odd one out\n",
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "\n",
    "#printing similarity index\n",
    "print(model.similarity('woman', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastText is an extension to Word2Vec proposed by Facebook in 2016. \n",
    "Instead of feeding individual words into the Neural Network, \n",
    "FastText breaks words into several n-grams (sub-words). \n",
    "\n",
    "For ex : the tri-grams for the word apple is app, ppl, and ple \n",
    "    (ignoring the starting and ending of boundaries of words). \n",
    "\n",
    "The word embedding vector for apple will be the sum of all these n-grams. \n",
    "After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset. \n",
    "Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import FastText\n",
    "\n",
    "# model_ted = FastText(sentences_ted, size=100, window=5, min_count=5, workers=4,sg=1)\n",
    "\n",
    "# print(model_ted.similarity('woman', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Word2Vec : \n",
    "takes texts as training data for a neural network. The resulting embedding captures whether words appear in similar contexts.                                                                                                        \n",
    "-GloVe : \n",
    "focuses on words co-occurrences matrix over the whole corpus. \n",
    "Its embeddings relate to the probabilities that two words appear together.                                                                                                      \n",
    "-FastText : \n",
    "improves on Word2Vec by taking word parts into account, too. \n",
    "This trick enables training of embeddings on smaller datasets and generalization to unknown words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}